{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fab677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from config import Config\n",
    "from utils import ensure_dirs, device\n",
    "from models.moe import MoEClassifier\n",
    "\n",
    "def entropy_regularization(weights, eps=1e-8):\n",
    "    # weights: [B,K]\n",
    "    ent = -torch.sum(weights * torch.log(weights + eps), dim=1).mean()\n",
    "    return ent\n",
    "\n",
    "def train():\n",
    "    cfg = Config()\n",
    "    dev = device()\n",
    "    ensure_dirs(cfg.out_root/\"checkpoints\")\n",
    "\n",
    "    Xtr = np.load(cfg.features_dir/\"X_train.npy\")\n",
    "    ytr = np.load(cfg.features_dir/\"y_train.npy\")\n",
    "    Xv  = np.load(cfg.features_dir/\"X_val.npy\")\n",
    "    yv  = np.load(cfg.features_dir/\"y_val.npy\")\n",
    "    Xt  = np.load(cfg.features_dir/\"X_test.npy\")\n",
    "    yt  = np.load(cfg.features_dir/\"y_test.npy\")\n",
    "\n",
    "    mask = np.load(cfg.features_dir/\"best_mask.npy\")\n",
    "    Xtr = Xtr[:, mask==1]\n",
    "    Xv  = Xv[:, mask==1]\n",
    "    Xt  = Xt[:, mask==1]\n",
    "\n",
    "    num_classes = int(max(ytr.max(), yv.max(), yt.max()) + 1)\n",
    "    in_dim = Xtr.shape[1]\n",
    "\n",
    "    tr_ds = TensorDataset(torch.tensor(Xtr, dtype=torch.float32), torch.tensor(ytr, dtype=torch.long))\n",
    "    v_ds  = TensorDataset(torch.tensor(Xv, dtype=torch.float32), torch.tensor(yv, dtype=torch.long))\n",
    "    te_ds = TensorDataset(torch.tensor(Xt, dtype=torch.float32), torch.tensor(yt, dtype=torch.long))\n",
    "\n",
    "    tr_dl = DataLoader(tr_ds, batch_size=cfg.moe_batch_size, shuffle=True)\n",
    "    v_dl  = DataLoader(v_ds, batch_size=cfg.moe_batch_size, shuffle=False)\n",
    "    te_dl = DataLoader(te_ds, batch_size=cfg.moe_batch_size, shuffle=False)\n",
    "\n",
    "    model = MoEClassifier(in_dim=in_dim, num_classes=num_classes, num_experts=cfg.num_experts).to(dev)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=cfg.moe_lr)\n",
    "\n",
    "    best_f1 = -1\n",
    "\n",
    "    for epoch in range(cfg.moe_epochs):\n",
    "        model.train()\n",
    "        pbar = tqdm(tr_dl, desc=f\"MoE Epoch {epoch+1}/{cfg.moe_epochs}\")\n",
    "        for xb, yb in pbar:\n",
    "            xb, yb = xb.to(dev), yb.to(dev)\n",
    "            logits, weights = model(xb)\n",
    "\n",
    "            loss_cls = F.cross_entropy(logits, yb)\n",
    "            # encourage non-collapsing gate via entropy reg\n",
    "            loss_ent = -entropy_regularization(weights)  # maximize entropy => minimize negative entropy\n",
    "\n",
    "            loss = loss_cls + cfg.entropy_reg * loss_ent\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            pbar.set_postfix({\"loss\": float(loss), \"cls\": float(loss_cls)})\n",
    "\n",
    "        # validate\n",
    "        model.eval()\n",
    "        all_pred, all_true = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in v_dl:\n",
    "                xb = xb.to(dev)\n",
    "                logits, _ = model(xb)\n",
    "                pred = logits.argmax(dim=1).cpu().numpy()\n",
    "                all_pred.append(pred)\n",
    "                all_true.append(yb.numpy())\n",
    "\n",
    "        all_pred = np.concatenate(all_pred)\n",
    "        all_true = np.concatenate(all_true)\n",
    "        f1 = f1_score(all_true, all_pred, average=\"macro\")\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            torch.save(model.state_dict(), cfg.moe_ckpt)\n",
    "\n",
    "        print(f\"Val F1: {f1:.4f} | Best: {best_f1:.4f}\")\n",
    "\n",
    "    # test\n",
    "    model.load_state_dict(torch.load(cfg.moe_ckpt, map_location=dev))\n",
    "    model.eval()\n",
    "    all_pred, all_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in te_dl:\n",
    "            xb = xb.to(dev)\n",
    "            logits, _ = model(xb)\n",
    "            pred = logits.argmax(dim=1).cpu().numpy()\n",
    "            all_pred.append(pred)\n",
    "            all_true.append(yb.numpy())\n",
    "\n",
    "    all_pred = np.concatenate(all_pred)\n",
    "    all_true = np.concatenate(all_true)\n",
    "    print(\"Test Accuracy:\", accuracy_score(all_true, all_pred))\n",
    "    print(\"Test Macro F1:\", f1_score(all_true, all_pred, average=\"macro\"))\n",
    "    print(\"Saved best MoE:\", cfg.moe_ckpt)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
